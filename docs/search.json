[
  {
    "objectID": "setup.html#google-colab-setup",
    "href": "setup.html#google-colab-setup",
    "title": "Setup",
    "section": "Google Colab Setup",
    "text": "Google Colab Setup"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Learning for Forest Inventories",
    "section": "",
    "text": "An introductory workshop for deep learning techniques for Enhanced Forest Inventories (EFIs) presented at the Canadian Cross-Country EFI Checkup.\nGet Started View Schedule"
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Deep Learning for Forest Inventories",
    "section": "Overview",
    "text": "Overview\nThis workshop covers a basic end-to-end deep learning workflow for EFI applications, covering an introduction into deep learning, to data prepartation, and finally a demo of model training, evaluation, and deployment. There will also be some examples of real world forest inventory deep learning applications showcasing some of the possibilities there are with these techniques.\n\nYou will learn\n\nWhat deep learning is and how it can be used for EFIs.\nHow to prepare and read in data for use in deep learning.\nBasics of using the PyTorch deep learning library.\nDeep learning based forest attribute feature extraction.\nTraining a deep learning model for classification tasks.\nValidation and testing of deep learning models."
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Deep Learning for Forest Inventories",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nComfortable with Python and Jupyter notebooks.\nGoogle Colab will be used for running the live demo."
  },
  {
    "objectID": "index.html#instructors",
    "href": "index.html#instructors",
    "title": "Deep Learning for Forest Inventories",
    "section": "Instructors",
    "text": "Instructors\n\n\n\n\n\n\n\nName\nAffiliation\n\n\n\n\nBrent A. Murray\nIntegrated Remote Sensing Studio. University of British Columbia\n\n\nHarry Seely\nIntegrated Remote Sensing Studio. University of British Columbia\n\n\nYuwei Cao\nIntegrated Remote Sensing Studio. University of British Columbia"
  },
  {
    "objectID": "04_validation.html#validating-deep-learning-predictions",
    "href": "04_validation.html#validating-deep-learning-predictions",
    "title": "Validation of Deep Learning Derived Forest Inventory Predictions",
    "section": "Validating Deep Learning Predictions",
    "text": "Validating Deep Learning Predictions"
  },
  {
    "objectID": "04_validation.html#examples-of-evaluation-metrics-and-plotting-figures",
    "href": "04_validation.html#examples-of-evaluation-metrics-and-plotting-figures",
    "title": "Validation of Deep Learning Derived Forest Inventory Predictions",
    "section": "Examples of Evaluation Metrics and Plotting Figures",
    "text": "Examples of Evaluation Metrics and Plotting Figures"
  },
  {
    "objectID": "02_data.html#data-preparation-for-use-in-deep-learning",
    "href": "02_data.html#data-preparation-for-use-in-deep-learning",
    "title": "Data Preparation and Loading",
    "section": "Data Preparation for Use in Deep Learning",
    "text": "Data Preparation for Use in Deep Learning"
  },
  {
    "objectID": "02_data.html#loading-data-in-pytorch",
    "href": "02_data.html#loading-data-in-pytorch",
    "title": "Data Preparation and Loading",
    "section": "Loading Data in PyTorch",
    "text": "Loading Data in PyTorch"
  },
  {
    "objectID": "01_intro.html",
    "href": "01_intro.html",
    "title": "Introduction to Deep Learning",
    "section": "",
    "text": "The Deep Learning Book"
  },
  {
    "objectID": "01_intro.html#relevant-resources",
    "href": "01_intro.html#relevant-resources",
    "title": "Introduction to Deep Learning",
    "section": "",
    "text": "The Deep Learning Book"
  },
  {
    "objectID": "01_intro.html#what-is-deep-learning",
    "href": "01_intro.html#what-is-deep-learning",
    "title": "Introduction to Deep Learning",
    "section": "What is Deep Learning?",
    "text": "What is Deep Learning?\nDeep learning is a branch of machine learning, and both are forms of artificial intelligence (AI). AI refers to methods that enable machines to mimic human behavior. Machine learning is a subset of AI that uses statistical techniques to allow machines to improve their performance through experience, while deep learning is a subset of machine learning that enables the computation of multi-layered neural networks. A neural network is a term that is used interchangeably with deep learning and we will use them both throughout this workshop.\n\n\n\n\n\n\nFigure 1: Different Types of Artificial Intelligence and Common Examples of Each\n\n\n\n\nComponents of a Deep Neural Network\nThere are several key components and terms commonly used to describe a neural network. The depth of a neural network refers to the number of layers in its architecture (the example in Figure 2 has five layers), whereas the width refers to the number of neurons within each layer.\n\n\n\n\n\n\nFigure 2: Components of a Neural Network\n\n\n\nThe most fundamental unit of a deep neural network is the neuron, which serves as the building block of any neural network. A neuron receives a set of inputs (\\(x_i\\)), weights (\\(w_i\\)) and sums them, while adding a bias term (\\(b\\)). These weights and biases are adjustable parameters that the network learns during training.\n\\[\n\\Large y = w_1x_1 + w_2x_2 + b\n\\]\nAnother important part of a neuron is called the activation. The output of the weighted sum (\\(y\\)) is passed through an activation function, which determines the strength of the output and whether the neuron becomes activated or not.\n\n\n\n\n\n\nFigure 3: Components of a Neuron\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere are many activation functions, and you are encouraged to research which one would be best for your specific use case.\n\n\n\n\nThe Prediction - Feedback Loop\nThe prediction-feedback loop is what makes learning possible as a deep learning model doesn’t just memorize the data, it adapts and generalizes to it. This allows a model to fine-tune how it sees the input data to improve its performance based on the goal and the task.\n\n\n\n\n\n\nFigure 4: Prediction-Feedback Loop\n\n\n\nThis prediction–feedback loop is driven by the goal of the loss function, which guides the model in adjusting its weights and biases to find the optimal solution. The loss function assesses the difference between the true and predicted labels, typically, with the goal to minimize the error between the two. There are different types of loss functions, each providing different feedback to the model. Choosing a loss function depends on the task (e.g., classification, regression), and ensuring that it is appropriate for the problem is a crucial step.\nOver a number of iterations, or epochs, the model adjusts its weights and biases based on the feedback received from the loss function. Eventually, the model can no longer improve given the input data, and at this point, the loss begins to stabilize, indicating that the model has reached its optimal performance.\n\n\n\n\n\n\nFigure 5: Model Adjusting Over Epochs\n\n\n\nThis iterative adjustment process is called model training and involves a training and a separate validation dataset. The training dataset is used to teach the model by updating its parameters, while the validation dataset assesses how well the model generalizes to unseen data. Monitoring performance on the validation set helps to detect overfitting (when a model learns the training data too closely and fails to perform well on new inputs) ensuring the final model remains both accurate and robust. We can monitor this using loss curves as seen in Figure 5 ensuring that the loss continues to drop for both the training and validation datasets.\nAfter the prediction-feedback loop (training and validation), a testing dataset is used to provide an unbiased evaluation of the model’s final performance. Unlike the validation set, which guides the model tuning during training, the test set is completely withheld until all adjustments are complete. This ensures that the performance metrics reflect how the model will behave on truly unseen data, offering a realistic measure of its generalization ability and reliability for real-world applications.\n\n\n\n\n\n\nNote\n\n\n\nIt is crucial that these three separate and distinct datasets (training, validation, and testing) are generated for training deep learning models. Mixing or reusing data between these datasets can lead to misleading performance metrics and poor generalization. Proper separation ensures that the model is evaluated fairly at each stage of development and that its reported accuracy truly reflects performance on new, unseen data."
  },
  {
    "objectID": "01_intro.html#challenges-with-deep-learning",
    "href": "01_intro.html#challenges-with-deep-learning",
    "title": "Introduction to Deep Learning",
    "section": "Challenges with Deep Learning",
    "text": "Challenges with Deep Learning\n\nImplemented in Python\n\n\nMany Models Use Linux"
  },
  {
    "objectID": "01_intro.html#frequently-asked-questions",
    "href": "01_intro.html#frequently-asked-questions",
    "title": "Introduction to Deep Learning",
    "section": "Frequently Asked Questions",
    "text": "Frequently Asked Questions\n\nHow does deep learning differ from machine learning or artifical intelligence?\n\n\nWhat makes deep learning deep?\n\n\nWhat are some advantages and disadvantages of deep learning?\n\n\n\nAdvantages\nDisadvantages\n\n\n\n\nCan produce more accurate results\nRequires A LOT of training data\n\n\n\n\n\nWhen should a deep learning model be used?\nAs the complexity of the data or the task increases deep learning becomes a more benifical option. Raw lidar data for example is a complex dataset and is well suited for deep learning tasks.\n\n\n\n\n\n\nFigure 9: Data and Task Complexity for Deep and Machine Learning\n\n\n\n\n\nWhat computing resources are required to run a deep learning model?"
  },
  {
    "objectID": "03_train.html#training-deep-learning-models-in-pytorch",
    "href": "03_train.html#training-deep-learning-models-in-pytorch",
    "title": "Training Deep Learning Models",
    "section": "Training Deep Learning Models in PyTorch",
    "text": "Training Deep Learning Models in PyTorch\n\nOptimizers\n\n\nLoss Functions"
  },
  {
    "objectID": "05_examples.html#biomass",
    "href": "05_examples.html#biomass",
    "title": "Examples of Deep Learning Forest Inventory Products",
    "section": "Biomass",
    "text": "Biomass"
  },
  {
    "objectID": "05_examples.html#tree-species-composition",
    "href": "05_examples.html#tree-species-composition",
    "title": "Examples of Deep Learning Forest Inventory Products",
    "section": "Tree Species Composition",
    "text": "Tree Species Composition"
  },
  {
    "objectID": "05_examples.html#individual-tree-species",
    "href": "05_examples.html#individual-tree-species",
    "title": "Examples of Deep Learning Forest Inventory Products",
    "section": "Individual Tree Species",
    "text": "Individual Tree Species"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Time\nTopic\nMaterials\n\n\n\n\n09:00–09:10\nWelcome & Introductions\nHome\n\n\n09:10–09:40\nIntroduction to Deep Learning\n1-Intro\n\n\n09:40–10:15\nData Preparation and Loading\n2-Data\n\n\n10:15–10:30\nBreak\nNA\n\n\n10:30–11:00\nTraining Deep Learning Models\n3-Train\n\n\n11:00–11:20\nValidating Deep Learning Models\n4-Validation\n\n\n11:20–11:50\nExamples of Deep Learning Forest Inventory Products\n5-Examples\n\n\n11:50–12:00\nQuestion & Answer\nNA"
  },
  {
    "objectID": "schedule.html#agenda",
    "href": "schedule.html#agenda",
    "title": "Schedule",
    "section": "",
    "text": "Time\nTopic\nMaterials\n\n\n\n\n09:00–09:10\nWelcome & Introductions\nHome\n\n\n09:10–09:40\nIntroduction to Deep Learning\n1-Intro\n\n\n09:40–10:15\nData Preparation and Loading\n2-Data\n\n\n10:15–10:30\nBreak\nNA\n\n\n10:30–11:00\nTraining Deep Learning Models\n3-Train\n\n\n11:00–11:20\nValidating Deep Learning Models\n4-Validation\n\n\n11:20–11:50\nExamples of Deep Learning Forest Inventory Products\n5-Examples\n\n\n11:50–12:00\nQuestion & Answer\nNA"
  },
  {
    "objectID": "02_data.html#relevant-resources",
    "href": "02_data.html#relevant-resources",
    "title": "Data Preparation and Loading",
    "section": "Relevant Resources",
    "text": "Relevant Resources"
  },
  {
    "objectID": "02_data.html#data-download",
    "href": "02_data.html#data-download",
    "title": "Data Preparation and Loading",
    "section": "Data download",
    "text": "Data download\nPRF 2018 sample plots have a 14.1 m radius (625 m2)\nThe height normalized single photon LiDAR and sample plot data for thr PRF can be downloaded below:\nSPL LiDAR: https://opendata.nfis.org/downloads/petawawa/Raster/LiDAR_2018/PRF_benchmarking_harmonized_2018_ALS.zip\nField Plots: https://opendata.nfis.org/downloads/petawawa/Vector/Forest%20Sample%20Plots/SPL2018_EFI_ground_plots.zip"
  },
  {
    "objectID": "02_data.html#extract-lidar-point-clouds-using-the-lidr-package",
    "href": "02_data.html#extract-lidar-point-clouds-using-the-lidr-package",
    "title": "Data Preparation and Loading",
    "section": "Extract LiDAR point clouds using the lidR package",
    "text": "Extract LiDAR point clouds using the lidR package\nSet filepaths and other global parameters\n\nPLOT_RADIUS = 14.1\n\nPLOT_COORDS_FPATH = 'data/SPL2018_EFI_ground_plots/SPL2018_EFI_ground_plots/PRF_SPL2018_EFI_plots_pts_wgs84.shp'\n\nPLOT_DATA_FPATH = 'data/SPL2018_EFI_ground_plots/SPL2018_EFI_ground_plots/PRF_CNL_SPL_CalibrationData_LiveDeadStems.xlsx'\n\nTREE_SP_CODES_FPATH = 'data/mnrf_sp_codes.csv'\n\nLIDAR_DIR = 'E:/PRF/3_tiled_norm'\n\nPLOT_PC_DIR = \"data/plot_point_clouds\"\n\nLABELS_FPATH = 'data/labels.csv'\n\nLoad required R packages\n\nlibrary(lidR)\nlibrary(here)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(mapview)\nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(reticulate)\n\nLoad the LiDAR and plot coordinates\n\n#Read the normalized las catalog\nctg &lt;- readLAScatalog(LIDAR_DIR)\n\n#Set some catalog processing parameters\nopt_select(ctg) &lt;- \"xyz\"\nopt_progress(ctg) &lt;- FALSE\n\n#Read the plot coordinates and ensure CRS is correct\nplot_centers &lt;- st_read(here(PLOT_COORDS_FPATH)) %&gt;%\n    st_transform(lidR::crs(ctg)) %&gt;%\n    st_zm() %&gt;%\n    dplyr::rename(plot_id = Plot)\n\nReading layer `PRF_SPL2018_EFI_plots_pts_wgs84' from data source \n  `D:\\Sync\\EFI_DL_Workshop_2026\\DeepLearningEFI\\data\\SPL2018_EFI_ground_plots\\SPL2018_EFI_ground_plots\\PRF_SPL2018_EFI_plots_pts_wgs84.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 249 features and 5 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 299051.1 ymin: 5089954 xmax: 315101 ymax: 5098580\nz_range:       zmin: 0 zmax: 0\nProjected CRS: WGS 84 / UTM zone 18N\n\n#Get vector representation of catalog area\nctg_sf &lt;- st_as_sf(lidR::as.spatial(ctg))\n\nmapview(ctg_sf, layer.name = 'LiDAR Tiles') + mapview(plot_centers, \n                                                      col.regions = 'red', \n                                                      color = 'black',\n                                                      layer.name = 'Ground Plots')\n\n\n\n\n\nDefine functions to process ALS data\n\nextract_pc &lt;- function(coords, ctg){\n\n      x = coords[1]\n      y = coords[2]\n\n      las &lt;- lidR::clip_circle(\n        las = ctg,\n        x = x,\n        y = y,\n        radius = PLOT_RADIUS)\n      \n      return(las)\n\n    }\n\nclean_pc &lt;- function(las) {\n  \n  las &lt;- lidR::filter_duplicates(las)\n  \n  las &lt;- lidR::classify_noise(las, sor(9,2))\n  \n  las &lt;- lidR::filter_poi(las, Classification != LASNOISE)\n  \n  return(las)\n}\n\npc_to_matrix &lt;- function(las){\n\n  pc &lt;- unname(st_coordinates(las))\n\n  return(pc)\n\n}\n\nnormalize_pc_xy &lt;- function(pc){\n\n  pc[, 1] &lt;- pc[, 1] - mean(pc[, 1])\n  pc[, 2] &lt;- pc[, 2] - mean(pc[, 2])\n\n  return(pc)\n}\n\ncalc_pc_zq95 &lt;- function(pc){\n\n    zq95 &lt;- quantile(pc[, 3], probs = 0.95)\n\n    return(zq95)\n\n}\n\nplot_pc &lt;- function(pc){\n\n  pc_df &lt;- as.data.frame(pc)\n  names(pc_df) &lt;- c('x', 'y', 'z')\n\n  p &lt;- plot_ly(pc_df, x = ~x, y = ~y, z = ~z, type = \"scatter3d\", mode = \"markers\",\n             marker = list(size = 3, color = ~z, colorscale = \"Jet\")) %&gt;%\n            layout(title = \"\",\n                  scene = list(\n                    xaxis = list(title = \"\", showgrid = FALSE, showticklabels = TRUE, ticks = \"outside\"),\n                    yaxis = list(title = \"\", showgrid = FALSE, showticklabels = TRUE, ticks = \"outside\"),\n                    zaxis = list(title = \"Z\", showgrid = FALSE, showticklabels = TRUE, ticks = \"outside\")\n                  ))\n\n  return(p)\n\n}\n\nwrite_pc_to_npy &lt;- function(pc, plot_id, pc_out_dir){    \n    \n    pc_np &lt;- np$array(pc, dtype = \"float32\")\n\n    np$save(file.path(here(pc_out_dir), paste0(plot_id, \".npy\")), pc_np)\n\n    return('')\n    \n    }\n\nExtract plot point clouds from full aquisition\n\n#TODO: Subset for testing, rm later\nplot_centers &lt;- plot_centers %&gt;% slice_head(n = 3)\n\n# Get a list of plot coordinates\ncoords &lt;- split(st_coordinates(plot_centers), \n                  seq_len(nrow(plot_centers)))\n\n# Extract point clouds and store in list\npc_ls &lt;- plyr::llply(coords, .fun = extract_pc, ctg = ctg)\n# Clean point clouds\npc_ls &lt;- plyr::llply(pc_ls, .fun = clean_pc)\n\n# Convert point clouds to matrices\npc_ls &lt;- plyr::llply(pc_ls, .fun = pc_to_matrix)\n\n# Normalize point clouds X & Y coordinates (leave Z)\npc_ls &lt;- plyr::llply(pc_ls, .fun = normalize_pc_xy)\n\n# Calculate zq95 for each point cloud to check alignment with plot data later\nals_derived_zq95_ls &lt;- plyr::llply(pc_ls, .fun = calc_pc_zq95)\n\n# Add plot IDs to pc list\nnames(pc_ls) &lt;- plot_centers$plot_id\n\n# Visualize a point cloud\nplot_pc(pc_ls[[3]])\n\n\n\n\n\nLoad tree measurements\n\n# Load plot level tree measurements\ntrees_df &lt;- read_excel(PLOT_DATA_FPATH, sheet = 'Tree') %&gt;%\n              dplyr::rename(plot_id = PlotName,\n                            sp_code = tree_spec)\n\n\nhead(trees_df)\n\n# A tibble: 6 × 27\n  sp_code plot_id TreeID TreeSpec Origin Status   DBH CrownClass QualityClass\n    &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;       \n1       1 PRF001      24        1 P      D       10.1 &lt;NA&gt;       &lt;NA&gt;        \n2       1 PRF001      46        1 P      D        9.9 &lt;NA&gt;       &lt;NA&gt;        \n3       2 PRF001      20        2 N      L       67.5 D          A           \n4       2 PRF001      50        2 N      L       57.9 D          U           \n5       1 PRF001      10        1 N      L       55.9 D          A           \n6       1 PRF001      71        1 N      L       51.5 D          U           \n# ℹ 18 more variables: DecayClass &lt;dbl&gt;, Ht &lt;dbl&gt;, HLF &lt;dbl&gt;, HtFlag &lt;chr&gt;,\n#   phf &lt;dbl&gt;, baha &lt;dbl&gt;, CD_ht &lt;dbl&gt;, Int_ht &lt;dbl&gt;, BA_all &lt;dbl&gt;,\n#   TPH_all &lt;dbl&gt;, codom &lt;chr&gt;, domht &lt;dbl&gt;, ht_meas &lt;dbl&gt;, stems &lt;dbl&gt;,\n#   mvol &lt;dbl&gt;, tvol &lt;dbl&gt;, biomass &lt;dbl&gt;, size &lt;chr&gt;\n\n\nCompare 95th height percentiles derived from LiDAR with the 95th percentile of measured tree heights\n\nals_derived_zq95_df &lt;- data.frame(plot_id = plot_centers$plot_id,\n                                  als_zq95 = as.numeric(als_derived_zq95_ls))\n\nfield_zq95_df &lt;- trees_df %&gt;%\n                filter(!is.na(ht_meas)) %&gt;%\n                group_by(plot_id) %&gt;%\n                summarize(field_zq95 = quantile(ht_meas, probs = 0.95))\n\nzq95_df &lt;- field_zq95_df %&gt;%\n              left_join(als_derived_zq95_df, by = 'plot_id')\n\nzq95_df %&gt;%\n  ggplot(aes(x = field_zq95, y = als_zq95)) + \n  geom_point() + \n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"red\") +\n  theme_minimal() + \n  coord_fixed(ratio = 1)\n\n\n\n\n\n\n\n\nAdd species information to trees data\n\n# Load tree species codes\ntree_sp_codes_df &lt;- read.csv(TREE_SP_CODES_FPATH) %&gt;%\n                      mutate(species = trimws(species))\n\n\n# Add species names to trees df\ntrees_df &lt;- trees_df %&gt;% left_join(tree_sp_codes_df, \n                                   by = 'sp_code') \n\n\n# View unique species\nsort(round(table(trees_df['species']) / nrow(trees_df) * 100, 2), \n     decreasing = TRUE)\n\nspecies\n          Balsam fir           White pine     Red (soft) maple \n               14.69                14.66                13.12 \n            Red pine            Jack pine              Red oak \n               10.96                 4.74                 4.65 \n        White spruce             Tamarack          Sugar maple \n                4.61                 3.56                 3.51 \n     Trembling aspen             Ironwood       American beech \n                3.16                 1.97                 1.86 \n           Black ash          White birch     Largetooth aspen \n                1.79                 1.74                 1.56 \n        Yellow birch             Basswood Northern white cedar \n                0.99                 0.68                 0.47 \n     Eastern hemlock            White ash         American elm \n                0.37                 0.18                 0.09 \n        Black cherry        Balsam poplar \n                0.09                 0.02 \n\n\nClassify each plot as either dominated by coniferous or deciduous species.\n\nget_tvol_d_perc &lt;- function(trees){\n\n  decid_tvol &lt;- trees %&gt;% \n                  filter(sp_type == 'd') %&gt;%\n                  pull(tvol) %&gt;%\n                  sum()\n\n  plot_tvol &lt;- trees %&gt;% \n                  pull(tvol) %&gt;%\n                  sum()\n\n  perc_tvol_d &lt;- decid_tvol / plot_tvol * 100\n\n  return(perc_tvol_d)\n\n  }\n\nperc_d_vec &lt;- vector(mode = 'numeric')\n\nfor(plot_id_i in unique(trees_df$plot_id)){\n\n  plot_i_trees &lt;- trees_df[trees_df['plot_id'] == plot_id_i, ]\n\n  perc_d_vec &lt;- append(perc_d_vec, get_tvol_d_perc(plot_i_trees))\n\n}\n\n# Assing plot to be either coniferous or deciduous dominant\nperc_decid_df &lt;- data.frame(plot_id = unique(trees_df$plot_id),\n                            perc_decid = perc_d_vec) %&gt;%\n                  mutate(dom_sp_type = if_else(perc_decid &gt; 75, 'decid', \n                                       if_else(perc_decid &gt; 25 , 'mixed', 'conif')))\n\n\nhist(perc_decid_df$perc_decid)\n\n\n\n\n\n\n\nhead(perc_decid_df)\n\n  plot_id perc_decid dom_sp_type\n1  PRF001  4.6434971       conif\n2  PRF002  0.6137523       conif\n3  PRF003 62.1149520       mixed\n4  PRF004  0.0000000       conif\n5  PRF005  0.0000000       conif\n6  PRF006  0.0000000       conif\n\n\nDivide data into training, validation, and testing\n\n# Establish the number of samples per split\nn_test &lt;- round(nrow(perc_decid_df) * 0.15, 0)\n\nn_train &lt;-  round(nrow(perc_decid_df) * 0.70, 0)\n\nset.seed(25)\ntest_df &lt;- perc_decid_df %&gt;%\n    slice_sample(n=n_test) %&gt;%\n    mutate(split = \"test\")\n  \nset.seed(25)\ntrain_df &lt;- perc_decid_df %&gt;%\n    anti_join(test_df, by = \"plot_id\") %&gt;%\n    slice_sample(n=n_train) %&gt;%\n    mutate(split = \"train\")\n  \nval_df &lt;- perc_decid_df %&gt;%\n    anti_join(train_df, by = \"plot_id\") %&gt;%\n    anti_join(test_df, by = \"plot_id\")\n\n# Check splits\n\ncat(\n  sprintf(\"N Train Samples: %d (%.1f%%)\\n\", nrow(train_df), 100 * nrow(train_df) / nrow(perc_decid_df)),\n  sprintf(\"N Val Samples:   %d (%.1f%%)\\n\", nrow(val_df), 100 * nrow(val_df) / nrow(perc_decid_df)),\n  sprintf(\"N Test Samples:  %d (%.1f%%)\\n\", nrow(test_df), 100 * nrow(test_df) / nrow(perc_decid_df))\n)\n\nN Train Samples: 174 (69.9%)\n N Val Samples:   38 (15.3%)\n N Test Samples:  37 (14.9%)\n\n# Combine data frames\nlabels_df &lt;- dplyr::bind_rows(train_df, val_df, test_df)\n\nhead(labels_df)\n\n  plot_id perc_decid dom_sp_type split\n1  PRF165 22.3787648       conif train\n2  PRF031 74.1692435       mixed train\n3  PRF306  0.0000000       conif train\n4  PRF060  0.0000000       conif train\n5  PRF087 60.6430403       mixed train\n6  PRF132  0.7207467       conif train\n\n\nExport preprocessed point clouds and labels\n\n# Ensure labels contain the same plots as the ALS point clouds\nlabels_df &lt;- labels_df %&gt;% filter(plot_id %in% names(pc_ls))\n\n# Import python numpy module using reticulate in quarto\nnp &lt;- reticulate::import(\"numpy\")\n\n# Export point clouds to numpy (.npy) files for use in python\nmapply(write_pc_to_npy, \n       pc = pc_ls,\n       plot_id = plot_centers$plot_id,\n       pc_out_dir=PLOT_PC_DIR)\n\nPRF208R  PRF320  PRF016 \n     \"\"      \"\"      \"\" \n\n# Export labels\nwrite.csv(labels_df, LABELS_FPATH, row.names=FALSE)"
  },
  {
    "objectID": "02_data.html#summary-of-preprocessing-steps",
    "href": "02_data.html#summary-of-preprocessing-steps",
    "title": "Data Preparation and Loading",
    "section": "Summary of Preprocessing Steps",
    "text": "Summary of Preprocessing Steps\n\nExtract ALS point clouds corresponding to ground reference plots\nNormalize X and Y coordinates\nCompare 95th height percentile of measured tree heights with ALS derived 95th height percentile\nDerive point cloud “labels”, establishing which plots are coniferous/deciduous/mixed\nSplit the data into training, validation, and test sets (i.e., splits)\nExport point clouds to numpy files for later processing"
  },
  {
    "objectID": "01_intro.html#image-based-vs.-point-based-deep-learning",
    "href": "01_intro.html#image-based-vs.-point-based-deep-learning",
    "title": "Introduction to Deep Learning",
    "section": "Image-Based vs. Point-Based Deep Learning",
    "text": "Image-Based vs. Point-Based Deep Learning\n\nImage-Based Deep Learning\nImage-based deep learning is a computer vision method that teaches computers to understand and interpret images. Instead of manually telling the computer what to look for, these algorithms automatically learn patterns from large sets of images. These algorithms can learn features like edges, shapes, and colours, and eventually can identify complex objects such as trees. Because these models learn directly from the data, they can help computers see and make sense of the imagery.\n\n\n\n\n\n\nFigure 6: Image Based Deep Learning\n\n\n\nA common image-based deep learning technique is the convolutional neural network (CNN). CNNs use filters (also called kernels) that slide across an image to detect important visual features. These filters help the network to focus on local patterns while keeping track of the spatial relationships in the image. As the data moves through the different layers of the network, the CNN learns increasingly complex patterns.\n\n\n\n\n\n\nFigure 7: Convolutional Neural Networks\n\n\n\n\n\nPoint Based Deep Learning\nPoint-based deep learning is a method used to analyze 3D data point clouds. Unlike image data, which is structured in a grid of pixels, point clouds are irregular and unorganized, making them harder for traditional neural networks to process. Point-based deep learning models are designed to handle this challenge by directly learning from the spatial relationships between points without needing to convert them into images or grids. By learning the geometric patterns directly from the raw point clouds, these models are able to provide an accurate and detailed understanding of complex 3D structures of the forest. Similar to image-based approaches as the data moves through the different layers different and more complex patterns can be extracted.\n\n\n\n\n\n\nFigure 8: Point Based Deep Learning"
  }
]