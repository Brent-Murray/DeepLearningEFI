---
title: "Data Preparation"
format:
  html:
    toc: True
execute:
  echo: true
---

![](https://opendata.nfis.org/mapserver/PRF_Layout.jpg){fig-alt="Petawawa Forest Map" caption="" width="80%"}

This section will provide preprocessing code to prepare LiDAR and sample plot data for subsequent point cloud deep learning model development.

## Dataset Background

The Petawawa Resereach Forest (PRF) is the oldest research forest in Canada, established in 1918. It is a remote sensing supersite and is part of the [GEO-TREES](https://geo-trees.org/) network.

In this workshop we make use of enhanced forest inventory sample plots collected in the PRF in 2018. We are also using single photon airborne laser scanning (ALS) data also collected in 2018.

The sample plots have a 14.1 m radius (625 m^2^).

The ALS data has a mean point density of 35 points/m^2^.

**The preprocessed data can be downloaded here:**

[https://ln5.sync.com/dl/88a6f5dd0#u24mrtkh-d5dqhart-zrg42shx-jfhhk8u5](https://ln5.sync.com/dl/88a6f5dd0#u24mrtkh-d5dqhart-zrg42shx-jfhhk8u5)

**The original height normalized ALS and sample plot data for the PRF can be downloaded from:**

SPL LiDAR: [https://opendata.nfis.org/downloads/petawawa/Raster/LiDAR_2018/PRF_benchmarking_harmonized_2018_ALS.zip](https://opendata.nfis.org/downloads/petawawa/Raster/LiDAR_2018/PRF_benchmarking_harmonized_2018_ALS.zip)

Field Plots: [https://opendata.nfis.org/downloads/petawawa/Vector/Forest%20Sample%20Plots/SPL2018_EFI_ground_plots.zip](https://opendata.nfis.org/downloads/petawawa/Vector/Forest%20Sample%20Plots/SPL2018_EFI_ground_plots.zip)

## Summary of Preprocessing Steps

1) Extract ALS point clouds corresponding to ground reference plots

2) Denoise point clouds and normalize X and Y coordinates

3) Export point clouds to numpy files for later use in model development

4) Compare 95th height percentile of measured tree heights with ALS derived 95th height percentile

5) Derive sample plot "labels", establishing which plots are coniferous/deciduous/mixed

6) Split the data into training, validation, and test sets (i.e., splits)


## Extract LiDAR point clouds using the lidR package

Set filepaths and other global parameters

```{r}

PROCESS_LIDAR = FALSE

PLOT_RADIUS = 14.1

PLOT_COORDS_FPATH = 'data/SPL2018_EFI_ground_plots/SPL2018_EFI_ground_plots/PRF_SPL2018_EFI_plots_pts_wgs84.shp'

PLOT_DATA_FPATH = 'data/SPL2018_EFI_ground_plots/SPL2018_EFI_ground_plots/PRF_CNL_SPL_CalibrationData_LiveDeadStems.xlsx'

TREE_SP_CODES_FPATH = 'data/mnrf_sp_codes.csv'

LIDAR_DIR = 'E:/PRF/3_tiled_norm'

PLOT_PC_DIR = "data/plot_point_clouds"

LABELS_FPATH = 'data/labels.csv'

ZQ95_FPATH = 'data/zq95.csv'

```


Load required R packages

```{r}

library(lidR)
library(here)
library(sf)
library(dplyr)
library(mapview)
library(readxl)
library(ggplot2)
library(plotly)
library(reticulate)

# Import python numpy module using reticulate
np <- reticulate::import("numpy")

```

Load the LiDAR and plot coordinates

```{r}

#Read the normalized las catalog
ctg <- readLAScatalog(LIDAR_DIR)

#Set some catalog processing parameters
opt_select(ctg) <- "xyz"
opt_progress(ctg) <- FALSE

#Read the plot coordinates and ensure CRS is correct
plot_centers <- st_read(here(PLOT_COORDS_FPATH)) %>%
    st_transform(lidR::crs(ctg)) %>%
    st_zm() %>%
    dplyr::rename(plot_id = Plot)

#Get vector representation of catalog area
ctg_sf <- st_as_sf(lidR::as.spatial(ctg))

mapview(ctg_sf, layer.name = 'LiDAR Tiles') + mapview(plot_centers, 
                                                      col.regions = 'red', 
                                                      color = 'black',
                                                      layer.name = 'Ground Plots')

```

Define functions to process ALS data

````{r}

extract_pc <- function(coords, ctg){

      x = coords[1]
      y = coords[2]

      las <- lidR::clip_circle(
        las = ctg,
        x = x,
        y = y,
        radius = PLOT_RADIUS)
      
      return(las)

    }

clean_pc <- function(las) {
  
  las <- lidR::filter_duplicates(las)
  
  las <- lidR::classify_noise(las, sor(9,2))
  
  las <- lidR::filter_poi(las, Classification != LASNOISE)
  
  return(las)
}

pc_to_matrix <- function(las){

  pc <- unname(st_coordinates(las))

  return(pc)

}

normalize_pc_xy <- function(pc){

  pc[, 1] <- pc[, 1] - mean(pc[, 1])
  pc[, 2] <- pc[, 2] - mean(pc[, 2])

  return(pc)
}

calc_pc_zq95 <- function(pc){

    zq95 <- quantile(pc[, 3], probs = 0.95)

    return(zq95)

}

plot_pc <- function(pc){

  pc_df <- as.data.frame(pc)
  names(pc_df) <- c('x', 'y', 'z')

  p <- plot_ly(pc_df, x = ~x, y = ~y, z = ~z, type = "scatter3d", mode = "markers",
             marker = list(size = 3, color = ~z, colorscale = "Jet")) %>%
            layout(title = "",
                  scene = list(
                    xaxis = list(title = "", showgrid = FALSE, showticklabels = TRUE, ticks = "outside"),
                    yaxis = list(title = "", showgrid = FALSE, showticklabels = TRUE, ticks = "outside"),
                    zaxis = list(title = "Z", showgrid = FALSE, showticklabels = TRUE, ticks = "outside")
                  ))

  return(p)

}

write_pc_to_npy <- function(pc, plot_id, pc_out_dir){    
    
    pc_np <- np$array(pc, dtype = "float32")

    np$save(file.path(here(pc_out_dir), paste0(plot_id, ".npy")), pc_np)

    return('')
    
    }

````

Extract plot point clouds from full aquisition

````{r}

if(PROCESS_LIDAR){

  # Get a list of plot coordinates
  coords <- split(st_coordinates(plot_centers), 
                    seq_len(nrow(plot_centers)))

  # Extract point clouds and store in list
  pc_ls <- plyr::llply(coords, .fun = extract_pc, ctg = ctg, .progress='text')

  # Clean point clouds
  pc_ls <- plyr::llply(pc_ls, .fun = clean_pc, .progress='text')

  # Convert point clouds to matrices
  pc_ls <- plyr::llply(pc_ls, .fun = pc_to_matrix, .progress='text')

  # Normalize point clouds X & Y coordinates (leave Z)
  pc_ls <- plyr::llply(pc_ls, .fun = normalize_pc_xy, .progress='text')

  # Calculate zq95 for each point cloud to check alignment with plot data later
  als_derived_zq95_ls <- plyr::llply(pc_ls, .fun = calc_pc_zq95, .progress='text')

  # Ensure the point cloud directory exists
  dir.create(PLOT_PC_DIR, recursive = TRUE, showWarnings = FALSE)

  # Export point clouds to numpy (.npy) files for use in python
  mapply(write_pc_to_npy, 
        pc = pc_ls,
        plot_id = plot_centers$plot_id,
        pc_out_dir=PLOT_PC_DIR)

  # Export ZQ95 values
  als_derived_zq95_df <- data.frame(plot_id = plot_centers$plot_id,
                                    als_zq95 = as.numeric(als_derived_zq95_ls))

  write.csv(als_derived_zq95_df, ZQ95_FPATH, row.names=FALSE)

}


````


````{r}

# Load an example pc
demo_pc <- np$load(list.files(PLOT_PC_DIR, full.names = TRUE)[50])

# Visualize a point cloud
plot_pc(demo_pc)

````


Load tree measurements

````{r}

# Load plot level tree measurements
trees_df <- read_excel(PLOT_DATA_FPATH, sheet = 'Tree') %>%
              dplyr::rename(plot_id = PlotName,
                            sp_code = tree_spec)


head(trees_df)
````

Compare 95th height percentiles derived from LiDAR with the 95th percentile of measured tree heights

````{r}

als_derived_zq95_df <- read.csv(ZQ95_FPATH)

field_zq95_df <- trees_df %>%
                filter(!is.na(ht_meas)) %>%
                group_by(plot_id) %>%
                summarize(field_zq95 = quantile(ht_meas, probs = 0.95))

zq95_df <- field_zq95_df %>%
              left_join(als_derived_zq95_df, by = 'plot_id')

zq95_df %>%
  ggplot(aes(x = field_zq95, y = als_zq95)) + 
  geom_point() + 
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  theme_minimal() + 
  coord_fixed(ratio = 1)

````

Add species information to trees data

````{r}

# Load tree species codes
tree_sp_codes_df <- read.csv(TREE_SP_CODES_FPATH) %>%
                      mutate(species = trimws(species))


# Add species names to trees df
trees_df <- trees_df %>% left_join(tree_sp_codes_df, 
                                   by = 'sp_code') 


# View unique species
sort(round(table(trees_df['species']) / nrow(trees_df) * 100, 2), 
     decreasing = TRUE)

````

Classify each plot as either dominated by coniferous or deciduous species.

````{r}

DOMINANCE_PERC_THRESH <- 70

get_tvol_d_perc <- function(trees){

  decid_tvol <- trees %>% 
                  filter(sp_type == 'd') %>%
                  pull(tvol) %>%
                  sum()

  plot_tvol <- trees %>% 
                  pull(tvol) %>%
                  sum()

  perc_tvol_d <- decid_tvol / plot_tvol * 100

  return(perc_tvol_d)

  }

perc_d_vec <- vector(mode = 'numeric')

for(plot_id_i in unique(trees_df$plot_id)){

  plot_i_trees <- trees_df[trees_df['plot_id'] == plot_id_i, ]

  perc_d_vec <- append(perc_d_vec, get_tvol_d_perc(plot_i_trees))

}

# Assing plot to be either coniferous or deciduous dominant
perc_decid_df <- data.frame(plot_id = unique(trees_df$plot_id),
                            perc_decid = perc_d_vec) %>%
                  mutate(dom_sp_type = if_else(perc_decid > DOMINANCE_PERC_THRESH, 'decid', 
                                       if_else(perc_decid > (100 - DOMINANCE_PERC_THRESH) , 'mixed', 'conif')))


hist(perc_decid_df$perc_decid)

head(perc_decid_df)

table(perc_decid_df$dom_sp_type)

````

Divide data into training, validation, and testing

````{r}

# Establish the number of samples per split
n_test <- round(nrow(perc_decid_df) * 0.15, 0)

n_train <-  round(nrow(perc_decid_df) * 0.70, 0)

set.seed(14)
test_df <- perc_decid_df %>%
    slice_sample(n=n_test) %>%
    mutate(split = "test")
  
set.seed(14)
train_df <- perc_decid_df %>%
    anti_join(test_df, by = "plot_id") %>%
    slice_sample(n=n_train) %>%
    mutate(split = "train")
  
val_df <- perc_decid_df %>%
    anti_join(train_df, by = "plot_id") %>%
    anti_join(test_df, by = "plot_id") %>%
    mutate(split = "val")


# Check splits
cat(
  sprintf("N Train Samples: %d (%.1f%%)\n", nrow(train_df), 100 * nrow(train_df) / nrow(perc_decid_df)),
  sprintf("N Val Samples:   %d (%.1f%%)\n", nrow(val_df), 100 * nrow(val_df) / nrow(perc_decid_df)),
  sprintf("N Test Samples:  %d (%.1f%%)\n", nrow(test_df), 100 * nrow(test_df) / nrow(perc_decid_df))
)

# Combine data frames
labels_df <- dplyr::bind_rows(train_df, val_df, test_df)

# View frequency by split
ggplot(labels_df, aes(x = split, fill = dom_sp_type)) +
  geom_bar(position = "dodge") +
  geom_text(stat = "count", aes(label = ..count..), 
            position = position_dodge(width = 0.9), vjust = -0.5, color = "black", size = 5) +
  labs(x = "Data Split",
       y = "Number of Observations",
       fill = "Dominant Species Type") +
  theme_minimal() +
  scale_fill_brewer(
    palette = "Set2",
    labels = c("conif" = "Coniferous", "decid" = "Deciduous", "mixed" = "Mixed")
  )

head(labels_df)

````

Export labels

````{r}

# Ensure that labels correspond to existing point cloud files
pc_flist <- list.files(PLOT_PC_DIR, full.names = TRUE)
pc_id_ls <- gsub(".npy", "", basename(pc_flist))

# Ensure labels contain the same plots as the ALS point clouds
labels_df <- labels_df %>% filter(plot_id %in% pc_id_ls)

# Export labels
write.csv(labels_df, LABELS_FPATH, row.names=FALSE)

````
